{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # export OMP_NUM_THREADS=1\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" # export OPENBLAS_NUM_THREADS=1\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\" # export MKL_NUM_THREADS=1\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\" # export VECLIB_MAXIMUM_THREADS=1\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" # export NUMEXPR_NUM_THREADS=1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= 'None'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multielec_src.fitting as fitting\n",
    "import multielec_src.multielec_utils as mutils\n",
    "import multielec_src.closed_loop as cl\n",
    "import multielec_src.old_labview_data_reader as oldlv\n",
    "from scipy.io import loadmat\n",
    "import multiprocessing as mp\n",
    "import statsmodels.api as sm\n",
    "from copy import deepcopy, copy\n",
    "import visionloader as vl\n",
    "from gsort.gsort_core_scripts import *\n",
    "from gsort.gsort_data_loader import *\n",
    "from itertools import product\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import collections\n",
    "from scipy.special import logsumexp\n",
    "from scipy.io import savemat\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSORT_BASE = \"/Volumes/Scratch/Analysis\"\n",
    "ESTIM_ANALYSIS_BASE = \"/Volumes/Scratch/Users/praful/pp_out_v2\"\n",
    "WNOISE_ANALYSIS_BASE = \"/Volumes/Acquisition/Analysis\"\n",
    "dataset = \"2023-10-30-0\"\n",
    "wnoise = \"data000\"\n",
    "spont_limit = 0.2\n",
    "min_inds = 2    # TODO: see how noisy the data is and tweak this parameter if needed\n",
    "ms = [1]\n",
    "\n",
    "estim_neg = \"data002\"\n",
    "\n",
    "outpath = os.path.join(GSORT_BASE, dataset, estim_neg, wnoise)\n",
    "electrical_path = os.path.join(ESTIM_ANALYSIS_BASE, dataset, estim_neg)\n",
    "parameters = loadmat(os.path.join(outpath, 'parameters.mat'))\n",
    "\n",
    "cells = parameters['cells'].flatten()\n",
    "num_cells = len(cells)\n",
    "num_patterns = max(parameters['patterns'].flatten())\n",
    "num_movies = parameters['movies'].flatten()[0]\n",
    "\n",
    "all_probs_neg = np.array(np.memmap(os.path.join(outpath, 'init_probs.dat'),mode='r',shape=(num_cells, num_patterns, num_movies), dtype='float32'))\n",
    "trials_neg = np.array(np.memmap(os.path.join(outpath, 'trial.dat'),mode='r',shape=(num_patterns, num_movies), dtype='int16'), dtype=int)\n",
    "amps_neg = np.array([mutils.get_stim_amps_newlv(electrical_path, 2).reshape(-1, 1)] * len(trials_neg))\n",
    "\n",
    "gsorted_cells = parameters['gsorted_cells'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcd = vl.load_vision_data(os.path.join(WNOISE_ANALYSIS_BASE, dataset, wnoise), wnoise.split('/')[-1],\n",
    "                          include_neurons=True,\n",
    "                          include_ei=True,\n",
    "                          include_params=True,\n",
    "                          include_noise=True)\n",
    "\n",
    "vcd.update_cell_type_classifications_from_text_file(os.path.join(WNOISE_ANALYSIS_BASE, dataset, wnoise, \n",
    "                                                                 'classification_deduped.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change this to parasol and midget if monkey\n",
    "\n",
    "gsorted_cells_new = []\n",
    "for i in range(len(gsorted_cells)):\n",
    "    if 'parasol' in vcd.get_cell_type_for_cell(cells[gsorted_cells[i]]).lower():\n",
    "        gsorted_cells_new.append(gsorted_cells[i])\n",
    "    if 'midget' in vcd.get_cell_type_for_cell(cells[gsorted_cells[i]]).lower():\n",
    "        gsorted_cells_new.append(gsorted_cells[i])\n",
    "\n",
    "gsorted_cells = np.array(gsorted_cells_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data below: do other stuff if your pipeline is different\n",
    "all_sigmoids = np.zeros((len(gsorted_cells), all_probs_neg.shape[1], all_probs_neg.shape[2]))\n",
    "\n",
    "jac_dict = collections.defaultdict(dict)\n",
    "transform_mat = []\n",
    "probs_vec = []\n",
    "num_params = 0\n",
    "\n",
    "for k in range(len(gsorted_cells)):\n",
    "    i = gsorted_cells[k]\n",
    "    for j in range(len(all_probs_neg[i])):\n",
    "        clean_inds = np.where(~np.isnan(all_probs_neg[i][j]))[0]\n",
    "        if len(clean_inds) == 0:\n",
    "            continue\n",
    "        probs = deepcopy(all_probs_neg[i][j][clean_inds])\n",
    "        X = deepcopy(amps_neg[j][clean_inds])\n",
    "        T = trials_neg[j][clean_inds]\n",
    "\n",
    "        if np.amax(probs) <= spont_limit:\n",
    "            all_sigmoids[k][j] = np.zeros(len(all_sigmoids[k][j]))\n",
    "\n",
    "        else:\n",
    "            X, probs, T = fitting.get_monotone_probs_and_amps(X, probs, T)\n",
    "\n",
    "            if np.count_nonzero(probs > spont_limit) < min_inds:\n",
    "                all_sigmoids[k][j] = np.full(len(all_sigmoids[k][j]), np.nan)\n",
    "            \n",
    "            else:\n",
    "                w_inits = []\n",
    "\n",
    "                for m in ms:\n",
    "                    w_init = np.array(np.random.normal(size=(m, amps_neg[j].shape[1]+1)))\n",
    "                    w_inits.append(w_init)\n",
    "                \n",
    "                opt, _ = fitting.fit_surface(X, probs, T, w_inits)\n",
    "                params, _, R2 = opt\n",
    "\n",
    "                all_sigmoids[k][j] = fitting.sigmoidND_nonlinear(\n",
    "                                        sm.add_constant(amps_neg[j], has_constant='add'), \n",
    "                                        params)\n",
    "                \n",
    "                X = jnp.array(sm.add_constant(amps_neg[j], has_constant='add'))\n",
    "                jac_dict[k][j] = jax.jacfwd(cl.activation_probs, argnums=1)(X, jnp.array(params)).reshape(\n",
    "                                                (len(X), params.shape[0]*params.shape[1]))\n",
    "                num_params += jac_dict[k][j].shape[1]\n",
    "\n",
    "                transform = jnp.zeros(len(trials_neg))\n",
    "                transform = transform.at[j].set(1)\n",
    "                transform_mat.append(transform)     # append a e-vector (512)\n",
    "\n",
    "                probs_vec.append(all_sigmoids[k][j])  # append a c-vector (80)\n",
    "\n",
    "\n",
    "                # TODO: uncomment this to look at some sigmoids and make sure they are reasonable\n",
    "                # print(cells[i], j+1)\n",
    "                # print(params, R2)\n",
    "            \n",
    "                # plt.figure(0)\n",
    "                # plt.xlim(-4.2, 0.1)\n",
    "                # plt.ylim(-0.1, 1.1)\n",
    "                # plt.scatter(amps_neg[j].flatten(), all_probs_neg[i][j])\n",
    "                # plt.plot(amps_neg[j].flatten(), all_sigmoids[k][j])\n",
    "                # plt.show()\n",
    "\n",
    "                # plt.figure(1)\n",
    "                # plt.scatter(np.arange(len(amps_neg[j])), all_probs_neg[i][j])\n",
    "                # plt.show()\n",
    "                # input()\n",
    "\n",
    "transform_mat = jnp.array(transform_mat, dtype='float32')\n",
    "probs_vec = jnp.array(jnp.hstack(probs_vec), dtype='float32')\n",
    "\n",
    "jac_full = jnp.zeros((len(probs_vec), num_params))\n",
    "counter_axis0 = 0\n",
    "counter_axis1 = 0\n",
    "for i in jac_dict.keys():\n",
    "    for j in jac_dict[i].keys():\n",
    "        next_jac = jac_dict[i][j]\n",
    "\n",
    "        jac_full = jac_full.at[counter_axis0:counter_axis0+next_jac.shape[0], counter_axis1:counter_axis1+next_jac.shape[1]].set(next_jac)\n",
    "\n",
    "        counter_axis0 += next_jac.shape[0]\n",
    "        counter_axis1 += next_jac.shape[1]\n",
    "\n",
    "jac_full = jnp.array(jac_full, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = jnp.clip(probs_vec, a_min=1e-5, a_max=1-1e-5) # need to clip these to prevent\n",
    "                                                            # overflow errors\n",
    "t = jnp.dot(transform_mat, trials_neg).flatten()\n",
    "I_p = t / (p_model * (1 - p_model))\n",
    "\n",
    "# Avoiding creating the large diagonal matrix and storing in memory\n",
    "I_w = jnp.dot((jac_full.T * I_p), jac_full) / len(p_model)\n",
    "\n",
    "# Avoiding multiplying the matrices out and calculating the trace explicitly\n",
    "sum_probs = jnp.sum(jnp.multiply(jac_full.T, jnp.linalg.solve(I_w, jac_full.T)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_mat_array = np.array(transform_mat)\n",
    "sum_probs_array = np.array(jnp.reshape(sum_probs, (-1, trials_neg.shape[1])))\n",
    "\n",
    "var_max = []\n",
    "var_LSE = []\n",
    "var_median = []\n",
    "var_sum = []\n",
    "var_mean = []\n",
    "for i in range(transform_mat_array.shape[1]):\n",
    "    cell_inds = np.where(transform_mat_array[:, i])[0]\n",
    "    if len(cell_inds > 0):\n",
    "        var_max.append(np.amax(sum_probs_array[cell_inds, :], axis=0))\n",
    "        var_LSE.append(logsumexp(sum_probs_array[cell_inds, :], axis=0))\n",
    "        var_median.append(np.median(sum_probs_array[cell_inds, :], axis=0))\n",
    "        var_sum.append(np.sum(sum_probs_array[cell_inds, :], axis=0))\n",
    "        var_mean.append(np.mean(sum_probs_array[cell_inds, :], axis=0))\n",
    "    else:\n",
    "        var_max.append(np.zeros(trials_neg.shape[1]))\n",
    "        var_LSE.append(np.zeros(trials_neg.shape[1]))\n",
    "        var_median.append(np.zeros(trials_neg.shape[1]))\n",
    "        var_sum.append(np.zeros(trials_neg.shape[1]))\n",
    "        var_mean.append(np.zeros(trials_neg.shape[1]))\n",
    "\n",
    "var_max = np.concatenate(var_max)\n",
    "var_LSE = np.concatenate(var_LSE)\n",
    "var_median = np.concatenate(var_median)\n",
    "var_sum = np.concatenate(var_sum)\n",
    "var_mean = np.concatenate(var_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_sum)\n",
    "plt.xlabel('Dictionary Element #')\n",
    "plt.ylabel('Total Variance of Element (summed across cells)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_mean)\n",
    "plt.xlabel('Dictionary Element #')\n",
    "plt.ylabel('Mean Variance of Element (across cells)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_median)\n",
    "plt.xlabel('Dictionary Element #')\n",
    "plt.ylabel('Median Variance of Element (across cells)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_max)\n",
    "plt.xlabel('Dictionary Element #')\n",
    "plt.ylabel('Max Variance of Element (across cells)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_LSE)\n",
    "plt.xlabel('Dictionary Element #')\n",
    "plt.ylabel('LogSumExp Variance of Element (across cells)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_low = 0.12\n",
    "estim_dict = all_sigmoids.reshape([all_sigmoids.shape[0], all_sigmoids.shape[1]*all_sigmoids.shape[2]])\n",
    "estim_dict[estim_dict < prob_low] = 0\n",
    "good_elements = np.where((~np.isnan(np.sum(estim_dict, axis=0))) & \n",
    "                         (np.amax(estim_dict, axis=0) > prob_low))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electrodes, amplitudes = np.unravel_index(good_elements, \n",
    "                                         (all_sigmoids.shape[1], all_sigmoids.shape[2]))\n",
    "EA = np.vstack((electrodes+1, amplitudes+1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estim_dict_pruned = estim_dict[:, good_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savemat(f'estim_dictionary_1elec_{dataset}-parasolmidget.mat', {'dictionary_matrix': estim_dict_pruned.T,\n",
    "                                                  'EA': EA})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estim_dict_pruned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvasi39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
